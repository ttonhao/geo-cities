# app_completo_sqlite.py
# type: ignore
"""
üöÄ SISTEMA AVAN√áADO DE DIST√ÇNCIAS - VERS√ÉO PRO COM SQLITE E CACHE DE DIST√ÇNCIAS
=============================================================================

VERS√ÉO COMPLETA: Sistema original + Cache SQLite + Cache de Dist√¢ncias + Melhorias
- ‚úÖ Cache SQLite persistente entre sess√µes
- ‚úÖ Cache de DIST√ÇNCIAS calculadas
- ‚úÖ Performance otimizada com √≠ndices
- ‚úÖ TTL autom√°tico e limpeza
- ‚úÖ Backup e restore autom√°tico
- ‚úÖ Processamento Paralelo completo  
- ‚úÖ Dashboard Analytics completo
- ‚úÖ Mapas Interativos completos
- ‚úÖ Painel de administra√ß√£o do banco
- ‚úÖ Corre√ß√£o de refresh da p√°gina
- üÜï Tracking detalhado de erros por origem-destino
- üÜï Ignora quando origem = destino

Vers√£o: 3.2.1 - Pro SQLite + Distance Cache + Error Tracking
"""

import streamlit as st
import pandas as pd
import requests
import time
import asyncio
import threading
import functools
import os
import sqlite3
import hashlib
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from geopy.geocoders import Nominatim
import plotly.express as px
import plotly.graph_objects as go
import folium
from streamlit_folium import st_folium
from io import BytesIO
from datetime import datetime, timedelta
import numpy as np
import warnings
import logging

# ================================
# CONFIGURA√á√ïES DE DEBUG E AMBIENTE
# ================================

DEBUG_MODE = os.getenv('STREAMLIT_ENV', 'production') == 'development'

class StreamlitDebugger:
    """Sistema de debug integrado para Streamlit"""
    
    def __init__(self, enabled=True):
        self.enabled = enabled and DEBUG_MODE
        self.session_logs = []
    
    def log(self, message, level="INFO", show_in_ui=False):
        """Log com op√ß√£o de mostrar na UI"""
        if not self.enabled:
            return
            
        timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
        thread_name = threading.current_thread().name
        log_entry = f"[{timestamp}] [{thread_name}] {level}: {message}"
        
        if DEBUG_MODE:
            print(log_entry)
        
        if 'debug_logs' not in st.session_state:
            st.session_state.debug_logs = []
        
        st.session_state.debug_logs.append({
            'timestamp': timestamp,
            'thread': thread_name,
            'level': level,
            'message': message
        })
        
        if len(st.session_state.debug_logs) > 1000:
            st.session_state.debug_logs = st.session_state.debug_logs[-500:]
    
    def debug(self, message, show_in_ui=False):
        self.log(message, "DEBUG", show_in_ui)
    
    def info(self, message, show_in_ui=False):
        self.log(message, "INFO", show_in_ui)
    
    def warning(self, message, show_in_ui=False):
        self.log(message, "WARNING", show_in_ui)
    
    def error(self, message, show_in_ui=False):
        self.log(message, "ERROR", show_in_ui)

debugger = StreamlitDebugger()

def debug_breakpoint(message="Debug breakpoint", variables=None):
    """Breakpoint customizado com informa√ß√µes contextuais"""
    if DEBUG_MODE:
        debugger.debug(f"BREAKPOINT: {message}")
        
        if variables:
            for name, value in variables.items():
                debugger.debug(f"  {name}: {value} (type: {type(value).__name__})")
        
        pass  # <-- COLOQUE BREAKPOINT AQUI

def debug_timing(func):
    """Decorator para medir tempo de execu√ß√£o com debug"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        if DEBUG_MODE:
            debugger.debug(f"Iniciando {func.__name__}")
        
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            if DEBUG_MODE:
                debugger.info(f"{func.__name__} executou em {execution_time:.3f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            debugger.error(f"{func.__name__} falhou ap√≥s {execution_time:.3f}s: {e}")
            raise
    
    return wrapper

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="üöÄ Sistema Avan√ßado de Dist√¢ncias - SQLite + Distance Cache",
    page_icon="üíæ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Suprimir warnings de threading
warnings.filterwarnings("ignore", message=".*missing ScriptRunContext.*")
logging.getLogger("streamlit.runtime.scriptrunner_utils.script_run_context").setLevel(logging.ERROR)

# CSS avan√ßado COMPLETO + novos estilos para SQLite
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 50%, #f093fb 100%);
        color: white;
        border-radius: 20px;
        margin-bottom: 2rem;
        box-shadow: 0 10px 40px rgba(0, 0, 0, 0.2);
        animation: gradient 3s ease infinite;
    }
    
    @keyframes gradient {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    
    .analytics-card {
        background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: white;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        backdrop-filter: blur(10px);
    }
    
    .performance-card {
        background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: #2c3e50;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
    }
    
    .cache-stats {
        background: linear-gradient(135deg, #d299c2 0%, #fef9d7 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 4px solid #9b59b6;
    }
    
    .sqlite-panel {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 15px;
        color: white;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
    }
    
    .distance-cache-panel {
        background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 4px solid #06d6a0;
    }
    
    .database-info {
        background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 4px solid #0066cc;
    }
    
    .backup-status {
        background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
        padding: 1rem;
        border-radius: 12px;
        color: #2c3e50;
        margin: 1rem 0;
    }
    
    .debug-panel {
        background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 4px solid #ff9500;
    }
    
    .error-panel {
        background: linear-gradient(135deg, #ff6b6b 0%, #feca57 100%);
        padding: 1rem;
        border-radius: 12px;
        margin: 1rem 0;
        border-left: 4px solid #e74c3c;
    }
</style>
""", unsafe_allow_html=True)

# ================================
# SISTEMA SQLITE CACHE COM DIST√ÇNCIAS
# ================================

# Vers√£o simplificada do cache SQLite com dist√¢ncias para demonstra√ß√£o
class SQLiteCacheWithDistances:
    """Cache SQLite com suporte a cache de dist√¢ncias"""
    
    def __init__(self, db_path="cache/geocoding_cache.db", ttl_hours=24, distance_ttl_hours=168):
        self.db_path = db_path
        self.ttl_hours = ttl_hours
        self.distance_ttl_hours = distance_ttl_hours  # 7 dias para dist√¢ncias
        self._ensure_db_directory()
        self._initialize_database()
        debugger.info(f"SQLite Cache com dist√¢ncias inicializado: {db_path}")
    
    def _ensure_db_directory(self):
        db_dir = os.path.dirname(self.db_path)
        if db_dir and not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
    
    def _initialize_database(self):
        conn = sqlite3.connect(self.db_path)
        
        # Tabela de coordenadas
        conn.execute("""
            CREATE TABLE IF NOT EXISTS coordinates (
                id INTEGER PRIMARY KEY,
                city_hash TEXT UNIQUE,
                city_name TEXT,
                longitude REAL,
                latitude REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP,
                hits INTEGER DEFAULT 0
            )
        """)
        
        # Nova tabela de dist√¢ncias
        conn.execute("""
            CREATE TABLE IF NOT EXISTS distances (
                id INTEGER PRIMARY KEY,
                route_hash TEXT UNIQUE,
                origin_city TEXT,
                destination_city TEXT,
                distance_km REAL,
                duration_minutes REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                expires_at TIMESTAMP,
                hits INTEGER DEFAULT 0,
                source TEXT DEFAULT 'osrm'
            )
        """)
        
        # Tabela de estat√≠sticas
        conn.execute("""
            CREATE TABLE IF NOT EXISTS cache_stats (
                date DATE PRIMARY KEY,
                coord_hits INTEGER DEFAULT 0,
                coord_misses INTEGER DEFAULT 0,
                coord_saves INTEGER DEFAULT 0,
                distance_hits INTEGER DEFAULT 0,
                distance_misses INTEGER DEFAULT 0,
                distance_saves INTEGER DEFAULT 0
            )
        """)
        
        # √çndices para performance
        conn.execute("CREATE INDEX IF NOT EXISTS idx_coordinates_hash ON coordinates(city_hash)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_distances_hash ON distances(route_hash)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_distances_expires ON distances(expires_at)")
        
        conn.commit()
        conn.close()
    
    def _get_route_hash(self, origin_city: str, destination_city: str) -> str:
        """Gera hash √∫nico para o par origem-destino"""
        origin_norm = origin_city.upper().strip()
        dest_norm = destination_city.upper().strip()
        
        # Ordem alfab√©tica para consist√™ncia (A->B = B->A)
        if origin_norm < dest_norm:
            route_string = f"{origin_norm}|{dest_norm}"
        else:
            route_string = f"{dest_norm}|{origin_norm}"
        
        return hashlib.sha256(route_string.encode('utf-8')).hexdigest()
    
    def save_coordinates(self, city_name, coordinates):
        city_hash = hashlib.md5(city_name.upper().encode()).hexdigest()
        expires_at = (datetime.now() + timedelta(hours=self.ttl_hours)).isoformat()
        
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            INSERT OR REPLACE INTO coordinates 
            (city_hash, city_name, longitude, latitude, expires_at)
            VALUES (?, ?, ?, ?, ?)
        """, (city_hash, city_name, coordinates[0], coordinates[1], expires_at))
        conn.commit()
        conn.close()
        
        self._update_stats('coord_saves')
        debugger.debug(f"SQLite: Coordenadas salvas - {city_name}")
    
    def get_coordinates(self, city_name):
        city_hash = hashlib.md5(city_name.upper().encode()).hexdigest()
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("""
            SELECT longitude, latitude, expires_at, hits 
            FROM coordinates WHERE city_hash = ?
        """, (city_hash,))
        
        row = cursor.fetchone()
        
        if row is None:
            conn.close()
            self._update_stats('coord_misses')
            debugger.debug(f"SQLite: Cache MISS coordenadas - {city_name}")
            return None
        
        # Verificar expira√ß√£o
        expires_at = datetime.fromisoformat(row[2])
        if datetime.now() > expires_at:
            conn.execute("DELETE FROM coordinates WHERE city_hash = ?", (city_hash,))
            conn.commit()
            conn.close()
            self._update_stats('coord_misses')
            debugger.debug(f"SQLite: Cache EXPIRED coordenadas - {city_name}")
            return None
        
        # Atualizar hits
        conn.execute("UPDATE coordinates SET hits = hits + 1 WHERE city_hash = ?", (city_hash,))
        conn.commit()
        conn.close()
        
        coordinates = [row[0], row[1]]
        self._update_stats('coord_hits')
        debugger.debug(f"SQLite: Cache HIT coordenadas - {city_name} -> {coordinates}")
        return coordinates
    
    def save_distance(self, origin_city: str, destination_city: str, distance_km: float, duration_minutes: float):
        """Salva dist√¢ncia calculada no cache"""
        route_hash = self._get_route_hash(origin_city, destination_city)
        expires_at = (datetime.now() + timedelta(hours=self.distance_ttl_hours)).isoformat()
        
        conn = sqlite3.connect(self.db_path)
        conn.execute("""
            INSERT OR REPLACE INTO distances 
            (route_hash, origin_city, destination_city, distance_km, duration_minutes, expires_at)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (route_hash, origin_city, destination_city, distance_km, duration_minutes, expires_at))
        conn.commit()
        conn.close()
        
        self._update_stats('distance_saves')
        debugger.debug(f"SQLite: Dist√¢ncia salva - {origin_city} -> {destination_city} = {distance_km}km")
    
    def get_distance(self, origin_city: str, destination_city: str):
        """Recupera dist√¢ncia do cache"""
        route_hash = self._get_route_hash(origin_city, destination_city)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("""
            SELECT distance_km, duration_minutes, expires_at, hits 
            FROM distances WHERE route_hash = ?
        """, (route_hash,))
        
        row = cursor.fetchone()
        
        if row is None:
            conn.close()
            self._update_stats('distance_misses')
            debugger.debug(f"SQLite: Cache MISS dist√¢ncia - {origin_city} -> {destination_city}")
            return None
        
        # Verificar expira√ß√£o
        expires_at = datetime.fromisoformat(row[2])
        if datetime.now() > expires_at:
            conn.execute("DELETE FROM distances WHERE route_hash = ?", (route_hash,))
            conn.commit()
            conn.close()
            self._update_stats('distance_misses')
            debugger.debug(f"SQLite: Cache EXPIRED dist√¢ncia - {origin_city} -> {destination_city}")
            return None
        
        # Atualizar hits
        conn.execute("UPDATE distances SET hits = hits + 1 WHERE route_hash = ?", (route_hash,))
        conn.commit()
        conn.close()
        
        result = {
            'distancia_km': round(row[0], 2),
            'tempo_min': round(row[1], 0),
            'status': 'cache_hit',
            'hits': row[3] + 1
        }
        
        self._update_stats('distance_hits')
        debugger.debug(f"SQLite: Cache HIT dist√¢ncia - {origin_city} -> {destination_city} = {result['distancia_km']}km")
        return result
    
    def _update_stats(self, action):
        conn = sqlite3.connect(self.db_path)
        today = datetime.now().date().isoformat()
        
        # Verificar se existe registro para hoje
        cursor = conn.execute("SELECT * FROM cache_stats WHERE date = ?", (today,))
        if cursor.fetchone() is None:
            conn.execute("INSERT INTO cache_stats (date) VALUES (?)", (today,))
        
        # Atualizar contador
        conn.execute(f"UPDATE cache_stats SET {action} = {action} + 1 WHERE date = ?", (today,))
        conn.commit()
        conn.close()
    
    def get_cache_stats(self):
        conn = sqlite3.connect(self.db_path)
        
        # Stats de hoje
        today = datetime.now().date().isoformat()
        cursor = conn.execute("""
            SELECT coord_hits, coord_misses, coord_saves, 
                   distance_hits, distance_misses, distance_saves 
            FROM cache_stats WHERE date = ?
        """, (today,))
        row = cursor.fetchone()
        
        if row is None:
            coord_hits = coord_misses = coord_saves = 0
            distance_hits = distance_misses = distance_saves = 0
        else:
            coord_hits, coord_misses, coord_saves, distance_hits, distance_misses, distance_saves = row
        
        # Stats globais
        cursor = conn.execute("SELECT COUNT(*) FROM coordinates WHERE expires_at > CURRENT_TIMESTAMP")
        total_coordinates = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(*) FROM distances WHERE expires_at > CURRENT_TIMESTAMP")
        total_distances = cursor.fetchone()[0]
        
        conn.close()
        
        # Calcular hit rates
        total_coord_requests = coord_hits + coord_misses
        coord_hit_rate = (coord_hits / total_coord_requests * 100) if total_coord_requests > 0 else 0
        
        total_distance_requests = distance_hits + distance_misses
        distance_hit_rate = (distance_hits / total_distance_requests * 100) if total_distance_requests > 0 else 0
        
        total_hits = coord_hits + distance_hits
        total_requests = total_coord_requests + total_distance_requests
        overall_hit_rate = (total_hits / total_requests * 100) if total_requests > 0 else 0
        
        file_size = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
        
        return {
            # Coordenadas
            'coord_hits': coord_hits,
            'coord_misses': coord_misses,
            'coord_saves': coord_saves,
            'coord_hit_rate': round(coord_hit_rate, 2),
            'total_coordinates': total_coordinates,
            
            # Dist√¢ncias
            'distance_hits': distance_hits,
            'distance_misses': distance_misses,
            'distance_saves': distance_saves,
            'distance_hit_rate': round(distance_hit_rate, 2),
            'total_distances': total_distances,
            
            # Geral
            'total_hits': total_hits,
            'total_requests': total_requests,
            'hit_rate': round(overall_hit_rate, 2),
            'total_entries': total_coordinates + total_distances,
            
            # Sistema
            'database_path': self.db_path,
            'file_size_bytes': file_size,
            'file_size_mb': round(file_size / (1024 * 1024), 2),
            'ttl_hours': self.ttl_hours,
            'distance_ttl_hours': self.distance_ttl_hours
        }
    
    def clear_cache(self):
        conn = sqlite3.connect(self.db_path)
        conn.execute("DELETE FROM coordinates")
        conn.execute("DELETE FROM distances")
        conn.execute("DELETE FROM cache_stats")
        conn.commit()
        conn.close()
        debugger.info("SQLite: Cache limpo completamente")
    
    def cleanup_expired(self):
        conn = sqlite3.connect(self.db_path)
        
        cursor = conn.execute("SELECT COUNT(*) FROM coordinates WHERE expires_at <= CURRENT_TIMESTAMP")
        expired_coords = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(*) FROM distances WHERE expires_at <= CURRENT_TIMESTAMP")
        expired_distances = cursor.fetchone()[0]
        
        conn.execute("DELETE FROM coordinates WHERE expires_at <= CURRENT_TIMESTAMP")
        conn.execute("DELETE FROM distances WHERE expires_at <= CURRENT_TIMESTAMP")
        conn.commit()
        conn.close()
        
        total_removed = expired_coords + expired_distances
        if total_removed > 0:
            debugger.info(f"SQLite: {expired_coords} coordenadas + {expired_distances} dist√¢ncias expiradas removidas")
        
        return total_removed
    
    def get_database_info(self):
        file_size = os.path.getsize(self.db_path) if os.path.exists(self.db_path) else 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.execute("SELECT COUNT(*) FROM coordinates")
        total_coords = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(*) FROM distances")
        total_distances = cursor.fetchone()[0]
        
        cursor = conn.execute("SELECT COUNT(*) FROM cache_stats")
        total_stats = cursor.fetchone()[0]
        
        conn.close()
        
        return {
            'database_path': self.db_path,
            'file_size_bytes': file_size,
            'file_size_mb': round(file_size / (1024 * 1024), 2),
            'total_coordinates': total_coords,
            'total_distances': total_distances,
            'total_stats_days': total_stats,
            'ttl_hours': self.ttl_hours,
            'distance_ttl_hours': self.distance_ttl_hours
        }

# ================================
# PROCESSAMENTO PARALELO COM CACHE DE DIST√ÇNCIAS E TRACKING DE ERROS
# ================================

class ParallelProcessor:
    def __init__(self, max_workers=4):
        self.max_workers = max_workers
        self.geocoder = Nominatim(user_agent="parallel_processor_error_tracking_v3", timeout=20) 
        
        # Usar cache SQLite com dist√¢ncias
        if DEBUG_MODE:
            self.cache = SQLiteCacheWithDistances("cache/dev_geocoding_cache.db", ttl_hours=1, distance_ttl_hours=24)
        else:
            self.cache = SQLiteCacheWithDistances("cache/prod_geocoding_cache.db", ttl_hours=24, distance_ttl_hours=168)
        
        debugger.info(f"ParallelProcessor inicializado com tracking de erros ({max_workers} workers)")
        
        # Migrar dados do cache em mem√≥ria se existir
        self._migrate_memory_cache()
    
    def _migrate_memory_cache(self):
        """Migra dados do cache em mem√≥ria antigo para SQLite"""
        if 'coordinates_cache' not in st.session_state:
            return
        
        memory_cache = st.session_state.coordinates_cache
        if not memory_cache:
            return
        
        debugger.info(f"Migrando {len(memory_cache)} entradas do cache em mem√≥ria para SQLite")
        migrated = 0
        
        for cache_hash, cached_data in memory_cache.items():
            try:
                city_name = cached_data.get('city', '')
                coordinates = cached_data.get('coords', [])
                
                if city_name and coordinates and len(coordinates) == 2:
                    self.cache.save_coordinates(city_name, coordinates)
                    migrated += 1
            except Exception as e:
                debugger.error(f"Erro na migra√ß√£o de {cached_data}: {e}")
        
        if migrated > 0:
            st.session_state.coordinates_cache = {}
            debugger.info(f"Migra√ß√£o conclu√≠da: {migrated} entradas transferidas para SQLite")
    
    @debug_timing
    def get_coordinates_with_cache(self, city_name):
        """Geocoding com cache SQLite"""
        # Tentar cache SQLite primeiro
        cached_coords = self.cache.get_coordinates(city_name)
        if cached_coords:
            return cached_coords
        
        debug_breakpoint("Geocoding com SQLite cache miss", {
            'city_name': city_name,
            'cache_type': 'SQLite'
        })
        
        try:
            location = self.geocoder.geocode(f"{city_name}, MG, Brasil")
            if location:
                coords = [location.longitude, location.latitude]
                debugger.info(f"Geocoding sucesso (MG): {city_name} -> {coords}")
                self.cache.save_coordinates(city_name, coords)
                return coords
            
            location = self.geocoder.geocode(f"{city_name}, Brasil")
            if location:
                coords = [location.longitude, location.latitude]
                debugger.info(f"Geocoding sucesso (BR): {city_name} -> {coords}")
                self.cache.save_coordinates(city_name, coords)
                return coords
            
            debugger.warning(f"Geocoding falhou: {city_name}")
            return None
            
        except Exception as e:
            debugger.error(f"Erro no geocoding de {city_name}: {e}")
            return None
    
    @debug_timing
    def calculate_distance_with_retry(self, origin_coords, dest_coords, origin_city, destination_city, max_retries=3):
        """Calcula dist√¢ncia com cache de dist√¢ncias e retry autom√°tico"""
        
        # *** NOVO: Verificar cache de dist√¢ncias primeiro ***
        cached_distance = self.cache.get_distance(origin_city, destination_city)
        if cached_distance:
            debugger.info(f"Cache HIT dist√¢ncia: {origin_city} -> {destination_city} = {cached_distance['distancia_km']}km")
            return cached_distance
        
        debugger.debug(f"Cache MISS dist√¢ncia - calculando: {origin_city} -> {destination_city}")
        
        # Calcular dist√¢ncia via API
        for attempt in range(max_retries):
            try:
                url = f"http://router.project-osrm.org/route/v1/driving/{origin_coords[0]},{origin_coords[1]};{dest_coords[0]},{dest_coords[1]}"
                params = {'overview': 'false', 'geometries': 'geojson'}
                
                debug_breakpoint("Calculando dist√¢ncia via API", {
                    'origin_city': origin_city,
                    'destination_city': destination_city,
                    'attempt': attempt + 1,
                    'cache_miss': True
                })
                
                response = requests.get(url, params=params, timeout=25)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    if data.get('routes'):
                        route = data['routes'][0]
                        distance_km = route['distance'] / 1000
                        duration_min = route['duration'] / 60
                        
                        result = {
                            'distancia_km': round(distance_km, 2),
                            'tempo_min': round(duration_min, 0),
                            'status': 'sucesso'
                        }
                        
                        # *** NOVO: Salvar no cache de dist√¢ncias ***
                        self.cache.save_distance(origin_city, destination_city, distance_km, duration_min)
                        
                        debugger.info(f"Dist√¢ncia calculada e salva: {origin_city} -> {destination_city} = {distance_km:.2f}km")
                        return result
                
                debugger.warning(f"Tentativa {attempt + 1} falhou: status {response.status_code}")
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # Backoff exponencial
                    
            except Exception as e:
                debugger.error(f"Erro na tentativa {attempt + 1}: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
        
        debugger.error(f"Todas as tentativas de c√°lculo falharam: {origin_city} -> {destination_city}")
        return {
            'status': 'erro_calculo', 
            'distancia_km': None, 
            'tempo_min': None,
            'erro_detalhes': f'Falha ao calcular rota {origin_city} -> {destination_city}'
        }
    
    @debug_timing
    def process_linha_paralela(self, linha_data):
        """Processa uma linha com cache SQLite, cache de dist√¢ncias e tracking detalhado de erros"""
        linha_excel = linha_data['linha_excel']
        origem = linha_data['origem']
        destinos = linha_data['destinos']
        
        debug_breakpoint("Processamento com tracking de erros", {
            'linha_excel': linha_excel,
            'origem': origem,
            'total_destinos': len(destinos),
            'cache_type': 'SQLite + Distance Cache + Error Tracking'
        })
        
        resultado = {
            'linha_excel': linha_excel,
            'origem': origem,
            'total_destinos': len(destinos),
            'destinos_calculados': [],
            'destino_mais_proximo': None,
            'km_mais_proximo': None,
            'sucessos': 0,
            'erros': 0,
            'tempo_processamento': 0,
            'status': 'processando',
            'erros_detalhados': []  # *** NOVO: Lista de erros espec√≠ficos ***
        }
        
        start_time = time.time()
        
        # Geocoding da origem com SQLite
        origin_coords = self.get_coordinates_with_cache(origem)
        
        if not origin_coords:
            resultado['status'] = 'origem_nao_encontrada'
            resultado['erros'] = len(destinos)
            # *** NOVO: Registrar erro espec√≠fico da origem ***
            resultado['erros_detalhados'].append({
                'tipo': 'geocoding_origem',
                'origem': origem,
                'destino': 'N/A',
                'erro': 'Coordenadas da origem n√£o encontradas'
            })
            debugger.error(f"Origem n√£o encontrada: {origem}")
            return resultado
        
        # Geocoding paralelo dos destinos
        def geocode_destino(destino):
            coords = self.get_coordinates_with_cache(destino)
            return destino, coords
        
        destinos_com_coords = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_destino = {executor.submit(geocode_destino, dest): dest for dest in destinos}
            
            for future in as_completed(future_to_destino):
                destino, coords = future.result()
                destinos_com_coords.append((destino, coords))
        
        # C√°lculo paralelo das dist√¢ncias COM CACHE E TRACKING DE ERROS
        def calcular_distancia_destino(item):
            destino, dest_coords = item
            
            if not dest_coords:
                # *** NOVO: Erro espec√≠fico de geocoding do destino ***
                erro_detalhado = {
                    'tipo': 'geocoding_destino',
                    'origem': origem,
                    'destino': destino,
                    'erro': 'Coordenadas do destino n√£o encontradas'
                }
                return {
                    'destino': destino,
                    'distancia_km': None,
                    'tempo_min': None,
                    'status': 'coordenadas_nao_encontradas',
                    'erro_detalhado': erro_detalhado
                }
            
            # *** PASSOU ORIGEM E DESTINO PARA O CACHE ***
            resultado_calc = self.calculate_distance_with_retry(origin_coords, dest_coords, origem, destino)
            resultado_calc['destino'] = destino
            
            # *** NOVO: Adicionar erro detalhado se falhou ***
            if resultado_calc['status'] not in ['sucesso', 'cache_hit']:
                erro_detalhado = {
                    'tipo': 'calculo_distancia',
                    'origem': origem,
                    'destino': destino,
                    'erro': resultado_calc.get('erro_detalhes', 'Erro no c√°lculo da dist√¢ncia via API')
                }
                resultado_calc['erro_detalhado'] = erro_detalhado
            
            time.sleep(0.1)
            return resultado_calc
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(destinos_com_coords))) as executor:
            future_to_calc = {executor.submit(calcular_distancia_destino, item): item for item in destinos_com_coords}
            
            for future in as_completed(future_to_calc):
                calc_resultado = future.result()
                resultado['destinos_calculados'].append(calc_resultado)
                
                if calc_resultado['status'] in ['sucesso', 'cache_hit']:
                    resultado['sucessos'] += 1
                else:
                    resultado['erros'] += 1
                    # *** NOVO: Adicionar erro detalhado √† lista ***
                    if 'erro_detalhado' in calc_resultado:
                        resultado['erros_detalhados'].append(calc_resultado['erro_detalhado'])
        
        # Identificar mais pr√≥ximo
        sucessos = [d for d in resultado['destinos_calculados'] if d['distancia_km'] is not None]
        
        if sucessos:
            mais_proximo = min(sucessos, key=lambda x: x['distancia_km'])
            resultado['destino_mais_proximo'] = mais_proximo['destino']
            resultado['km_mais_proximo'] = mais_proximo['distancia_km']
        
        resultado['tempo_processamento'] = round(time.time() - start_time, 2)
        resultado['status'] = 'concluido'
        
        # *** LOG DOS ERROS DETALHADOS ***
        if resultado['erros_detalhados']:
            debugger.warning(f"Linha {linha_excel} - {len(resultado['erros_detalhados'])} erros espec√≠ficos registrados")
            for erro in resultado['erros_detalhados']:
                debugger.error(f"Erro {erro['tipo']}: {erro['origem']} -> {erro['destino']} - {erro['erro']}")
        
        return resultado
    
    @debug_timing
    def process_all_lines_parallel(self, linhas_processaveis):
        """Processa todas as linhas em paralelo com cache de dist√¢ncias e tracking de erros"""
        start_time = time.time()
        
        resultados = []
        
        with ThreadPoolExecutor(max_workers=min(self.max_workers, len(linhas_processaveis))) as executor:
            future_to_linha = {
                executor.submit(self.process_linha_paralela, linha): linha 
                for linha in linhas_processaveis
            }
            
            for future in as_completed(future_to_linha):
                resultado = future.result()
                resultados.append(resultado)
        
        resultados.sort(key=lambda x: x['linha_excel'])
        
        # *** NOVO: Agregar erros de todas as linhas ***
        todos_erros = []
        for resultado in resultados:
            todos_erros.extend(resultado.get('erros_detalhados', []))
        
        stats_globais = {
            'tempo_total': round(time.time() - start_time, 2),
            'linhas_processadas': len(resultados),
            'total_sucessos': sum(r['sucessos'] for r in resultados),
            'total_erros': sum(r['erros'] for r in resultados),
            'media_tempo_por_linha': round(sum(r['tempo_processamento'] for r in resultados) / len(resultados), 2),
            'processamento_paralelo': True,
            'workers_utilizados': self.max_workers,
            'cache_type': 'SQLite + Distance Cache',
            'erros_detalhados': todos_erros  # *** NOVO: Lista completa de erros ***
        }
        
        # *** LOG RESUMO DOS ERROS ***
        if todos_erros:
            debugger.warning(f"Processamento conclu√≠do com {len(todos_erros)} erros espec√≠ficos")
            
            # Contar tipos de erro
            tipos_erro = {}
            for erro in todos_erros:
                tipo = erro['tipo']
                tipos_erro[tipo] = tipos_erro.get(tipo, 0) + 1
            
            for tipo, count in tipos_erro.items():
                debugger.warning(f"  - {tipo}: {count} ocorr√™ncias")
        
        return resultados, stats_globais

# ================================
# DASHBOARD DE ANALYTICS COMPLETO
# ================================

class AnalyticsDashboard:
    def __init__(self):
        self.init_session_analytics()
    
    def init_session_analytics(self):
        if 'analytics' not in st.session_state:
            st.session_state.analytics = {
                'processamentos': [],
                'performance_history': [],
                'cache_history': [],
                'error_log': [],
                'session_start': datetime.now()
            }
    
    def log_processamento(self, resultados, stats_globais):
        log_entry = {
            'timestamp': datetime.now(),
            'linhas_processadas': stats_globais['linhas_processadas'],
            'total_sucessos': stats_globais['total_sucessos'],
            'total_erros': stats_globais['total_erros'],
            'tempo_total': stats_globais['tempo_total'],
            'taxa_sucesso': (stats_globais['total_sucessos'] / max(1, stats_globais['total_sucessos'] + stats_globais['total_erros'])) * 100,
            'processamento_paralelo': stats_globais.get('processamento_paralelo', False),
            'workers': stats_globais.get('workers_utilizados', 1),
            'cache_type': stats_globais.get('cache_type', 'Memory'),
            'erros_detalhados': len(stats_globais.get('erros_detalhados', []))  # *** NOVO ***
        }
        
        st.session_state.analytics['processamentos'].append(log_entry)
        
        if len(st.session_state.analytics['processamentos']) > 10:
            st.session_state.analytics['processamentos'] = st.session_state.analytics['processamentos'][-10:]
    
    def show_performance_dashboard(self):
        st.markdown("### üìä Dashboard de Analytics")
        
        analytics = st.session_state.analytics
        
        if analytics['processamentos']:
            col1, col2, col3, col4 = st.columns(4)
            
            total_processamentos = len(analytics['processamentos'])
            ultimo_processamento = analytics['processamentos'][-1]
            media_tempo = sum(p['tempo_total'] for p in analytics['processamentos']) / total_processamentos
            media_taxa_sucesso = sum(p['taxa_sucesso'] for p in analytics['processamentos']) / total_processamentos
            
            with col1:
                st.metric("üîÑ Processamentos", total_processamentos)
            with col2:
                st.metric("‚è±Ô∏è Tempo M√©dio", f"{media_tempo:.1f}s")
            with col3:
                st.metric("üìà Taxa Sucesso M√©dia", f"{media_taxa_sucesso:.1f}%")
            with col4:
                cache_type = ultimo_processamento.get('cache_type', 'Memory')
                st.metric("üíæ Cache Type", cache_type)
            
            # *** NOVO: Mostrar info sobre erros detalhados ***
            total_erros_detalhados = sum(p.get('erros_detalhados', 0) for p in analytics['processamentos'])
            if total_erros_detalhados > 0:
                st.info(f"üîç **Tracking de Erros Ativo** - {total_erros_detalhados} erros detalhados registrados")
            
            # Mostrar se est√° usando cache de dist√¢ncias
            if 'Distance Cache' in ultimo_processamento.get('cache_type', ''):
                st.success("‚úÖ **Cache de Dist√¢ncias ativo** - Rotas j√° calculadas s√£o reutilizadas!")
            
            # Gr√°ficos de performance
            st.markdown("#### üìà Performance ao Longo do Tempo")
            
            df_performance = pd.DataFrame(analytics['processamentos'])
            df_performance['processamento_id'] = range(1, len(df_performance) + 1)
            
            col1, col2 = st.columns(2)
            
            with col1:
                fig_tempo = px.line(
                    df_performance,
                    x='processamento_id',
                    y='tempo_total',
                    title="‚è±Ô∏è Tempo de Processamento",
                    labels={'tempo_total': 'Tempo (segundos)', 'processamento_id': 'Processamento #'}
                )
                fig_tempo.update_traces(line_color='#667eea')
                st.plotly_chart(fig_tempo, use_container_width=True)
            
            with col2:
                fig_sucesso = px.line(
                    df_performance,
                    x='processamento_id',
                    y='taxa_sucesso',
                    title="üìà Taxa de Sucesso",
                    labels={'taxa_sucesso': 'Taxa de Sucesso (%)', 'processamento_id': 'Processamento #'}
                )
                fig_sucesso.update_traces(line_color='#764ba2')
                st.plotly_chart(fig_sucesso, use_container_width=True)
        
        else:
            st.info("üìä Execute um processamento para ver analytics detalhados")
    
    def show_cache_analytics(self, cache_stats):
        st.markdown("#### üíæ Analytics do Cache SQLite + Dist√¢ncias")
        
        # M√©tricas de coordenadas
        st.markdown("##### üìç Cache de Coordenadas")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("üéØ Coord Hits", cache_stats['coord_hits'])
        with col2:
            st.metric("‚ùå Coord Misses", cache_stats['coord_misses'])
        with col3:
            st.metric("üíæ Coord Saves", cache_stats['coord_saves'])
        with col4:
            st.metric("üìà Coord Hit Rate", f"{cache_stats['coord_hit_rate']:.1f}%")
        
        # M√©tricas de dist√¢ncias
        st.markdown("##### üõ£Ô∏è Cache de Dist√¢ncias")
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("üéØ Dist Hits", cache_stats['distance_hits'])
        with col2:
            st.metric("‚ùå Dist Misses", cache_stats['distance_misses'])
        with col3:
            st.metric("üíæ Dist Saves", cache_stats['distance_saves'])
        with col4:
            st.metric("üìà Dist Hit Rate", f"{cache_stats['distance_hit_rate']:.1f}%")
        
        # Informa√ß√µes gerais
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("üóÉÔ∏è Total Coordenadas", cache_stats['total_coordinates'])
        with col2:
            st.metric("üõ£Ô∏è Total Dist√¢ncias", cache_stats['total_distances'])
        with col3:
            st.metric("üíΩ Tamanho DB", f"{cache_stats.get('file_size_mb', 0):.2f} MB")
        
        # Gr√°fico de distribui√ß√£o geral
        if cache_stats['total_requests'] > 0:
            fig_cache = go.Figure(data=[
                go.Pie(
                    labels=['Cache Hits (Geral)', 'Cache Misses (Geral)'],
                    values=[cache_stats['total_hits'], cache_stats['total_requests'] - cache_stats['total_hits']],
                    hole=0.4,
                    marker_colors=['#11998e', '#e74c3c']
                )
            ])
            fig_cache.update_layout(
                title="üíæ Performance Geral do Cache (Coordenadas + Dist√¢ncias)",
                height=300
            )
            st.plotly_chart(fig_cache, use_container_width=True)

# ================================
# PAINEL DE ADMINISTRA√á√ÉO SQLITE COM DIST√ÇNCIAS
# ================================

def show_sqlite_admin_panel(cache_system):
    """Painel de administra√ß√£o avan√ßado do SQLite com cache de dist√¢ncias"""
    
    st.markdown('<div class="sqlite-panel">', unsafe_allow_html=True)
    st.markdown("### üíæ Administra√ß√£o do Banco SQLite + Dist√¢ncias")
    
    # Informa√ß√µes do banco
    db_info = cache_system.get_database_info()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("üìÅ Arquivo DB", os.path.basename(db_info['database_path']))
        st.metric("üíΩ Tamanho", f"{db_info['file_size_mb']:.2f} MB")
    
    with col2:
        st.metric("üóÉÔ∏è Coordenadas", db_info['total_coordinates'])
        st.metric("üõ£Ô∏è Dist√¢ncias", db_info['total_distances'])
    
    with col3:
        st.metric("‚è∞ TTL Coords", f"{db_info['ttl_hours']}h")
        st.metric("‚è∞ TTL Dist", f"{db_info['distance_ttl_hours']}h")
    
    # Painel espec√≠fico do cache de dist√¢ncias
    st.markdown('<div class="distance-cache-panel">', unsafe_allow_html=True)
    st.markdown("#### üõ£Ô∏è Cache de Dist√¢ncias")
    
    cache_stats = cache_system.get_cache_stats()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìä Hit Rate Dist√¢ncias", f"{cache_stats['distance_hit_rate']:.1f}%")
    with col2:
        st.metric("üíæ Dist√¢ncias Salvas", cache_stats['total_distances'])
    with col3:
        st.metric("‚è∞ TTL", f"{cache_stats['distance_ttl_hours']}h")
    
    st.info("üöÄ **Cache de Dist√¢ncias ativo** - Rotas j√° calculadas s√£o reutilizadas automaticamente!")
    st.markdown('</div>', unsafe_allow_html=True)
    
    # A√ß√µes de administra√ß√£o
    st.markdown("#### üõ†Ô∏è A√ß√µes de Administra√ß√£o")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üßπ Limpar Expirados", help="Remove entradas expiradas"):
            with st.spinner("Limpando entradas expiradas..."):
                removed = cache_system.cleanup_expired()
            
            if removed > 0:
                st.success(f"‚úÖ {removed} entradas expiradas removidas")
            else:
                st.info("‚ÑπÔ∏è Nenhuma entrada expirada encontrada")
    
    with col2:
        if st.button("üóëÔ∏è Limpar Cache", help="Remove todo o cache"):
            if st.checkbox("‚ö†Ô∏è Confirmar limpeza total"):
                cache_system.clear_cache()
                st.success("‚úÖ Cache limpo completamente")
    
    with col3:
        if st.button("üìä Atualizar Stats", help="Atualiza estat√≠sticas"):
            pass
    
    # Informa√ß√µes t√©cnicas
    with st.expander("üîß Informa√ß√µes T√©cnicas Detalhadas"):
        st.json(db_info)
    
    st.markdown('</div>', unsafe_allow_html=True)

# ================================
# NOVA FUN√á√ÉO PARA EXIBIR ERROS DETALHADOS
# ================================

def show_detailed_errors(erros_detalhados):
    """Exibe painel de erros detalhados"""
    if not erros_detalhados:
        return
    
    st.markdown('<div class="error-panel">', unsafe_allow_html=True)
    st.markdown("### üö® Erros Detalhados por Origem-Destino")
    
    # Estat√≠sticas dos erros
    col1, col2, col3 = st.columns(3)
    
    total_erros = len(erros_detalhados)
    tipos_erro = {}
    origens_com_erro = set()
    destinos_com_erro = set()
    
    for erro in erros_detalhados:
        tipo = erro['tipo']
        tipos_erro[tipo] = tipos_erro.get(tipo, 0) + 1
        origens_com_erro.add(erro['origem'])
        destinos_com_erro.add(erro['destino'])
    
    with col1:
        st.metric("üö® Total de Erros", total_erros)
    with col2:
        st.metric("üè† Origens Afetadas", len(origens_com_erro))
    with col3:
        st.metric("üéØ Destinos Afetados", len(destinos_com_erro))
    
    # Distribui√ß√£o dos tipos de erro
    st.markdown("#### üìä Distribui√ß√£o por Tipo de Erro")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        if tipos_erro:
            df_erros = pd.DataFrame([
                {'Tipo de Erro': tipo, 'Quantidade': qtd}
                for tipo, qtd in tipos_erro.items()
            ])
            
            fig_erros = px.bar(
                df_erros,
                x='Tipo de Erro',
                y='Quantidade',
                title="Tipos de Erro",
                color='Quantidade',
                color_continuous_scale='Reds'
            )
            st.plotly_chart(fig_erros, use_container_width=True)
    
    with col2:
        st.markdown("**Tipos de Erro:**")
        for tipo, qtd in tipos_erro.items():
            emoji = {
                'geocoding_origem': 'üè†',
                'geocoding_destino': 'üéØ',
                'calculo_distancia': 'üõ£Ô∏è'
            }.get(tipo, '‚ùå')
            st.metric(f"{emoji} {tipo.replace('_', ' ').title()}", qtd)
    
    # Tabela detalhada de erros
    st.markdown("#### üìã Lista Detalhada de Erros")
    
    # Preparar dados para a tabela
    df_erros_detalhados = pd.DataFrame(erros_detalhados)
    df_erros_detalhados['par_origem_destino'] = df_erros_detalhados['origem'] + ' ‚Üí ' + df_erros_detalhados['destino']
    
    # Adicionar emoji por tipo
    df_erros_detalhados['tipo_emoji'] = df_erros_detalhados['tipo'].map({
        'geocoding_origem': 'üè† Geocoding Origem',
        'geocoding_destino': 'üéØ Geocoding Destino',
        'calculo_distancia': 'üõ£Ô∏è C√°lculo Dist√¢ncia'
    })
    
    # Filtros para a tabela
    col1, col2 = st.columns(2)
    
    with col1:
        filtro_tipo = st.selectbox(
            "Filtrar por Tipo:",
            options=['Todos'] + list(tipos_erro.keys()),
            index=0
        )
    
    with col2:
        filtro_origem = st.selectbox(
            "Filtrar por Origem:",
            options=['Todas'] + sorted(list(origens_com_erro)),
            index=0
        )
    
    # Aplicar filtros
    df_filtrado = df_erros_detalhados.copy()
    
    if filtro_tipo != 'Todos':
        df_filtrado = df_filtrado[df_filtrado['tipo'] == filtro_tipo]
    
    if filtro_origem != 'Todas':
        df_filtrado = df_filtrado[df_filtrado['origem'] == filtro_origem]
    
    # Exibir tabela
    if not df_filtrado.empty:
        df_display = df_filtrado[['tipo_emoji', 'par_origem_destino', 'erro']].copy()
        df_display.columns = ['Tipo', 'Origem ‚Üí Destino', 'Descri√ß√£o do Erro']
        
        st.dataframe(
            df_display,
            use_container_width=True,
            hide_index=True,
            height=min(400, len(df_display) * 35 + 50)
        )
        
        st.info(f"üìä Mostrando {len(df_filtrado)} de {total_erros} erros")
    else:
        st.warning("üîç Nenhum erro encontrado com os filtros aplicados")
    
    st.markdown('</div>', unsafe_allow_html=True)

# ================================
# MAPAS INTERATIVOS
# ================================

class InteractiveMapper:
    def __init__(self):
        pass
    
    def create_route_map(self, resultado_linha):
        if resultado_linha['status'] != 'concluido':
            return None
        
        origem = resultado_linha['origem']
        sucessos = [d for d in resultado_linha['destinos_calculados'] if d['distancia_km'] is not None]
        
        if not sucessos:
            return None
        
        origem_lat, origem_lon = -21.7587, -43.3496
        
        m = folium.Map(
            location=[origem_lat, origem_lon],
            zoom_start=8,
            tiles='OpenStreetMap'
        )
        
        folium.Marker(
            [origem_lat, origem_lon],
            popup=f"üéØ ORIGEM: {origem}",
            tooltip=f"Origem: {origem}",
            icon=folium.Icon(color='red', icon='home', prefix='fa')
        ).add_to(m)
        
        cores = ['blue', 'green', 'orange', 'purple', 'pink', 'gray', 'darkblue', 'darkgreen']
        
        for i, destino_data in enumerate(sucessos[:10]):
            dest_lat = origem_lat + (np.random.random() - 0.5) * 2
            dest_lon = origem_lon + (np.random.random() - 0.5) * 2
            
            cor = cores[i % len(cores)]
            
            # Indicar se veio do cache
            cache_icon = "üíæ" if destino_data.get('status') == 'cache_hit' else "üÜï"
            
            folium.Marker(
                [dest_lat, dest_lon],
                popup=f"üìç {destino_data['destino']}<br>üìè {destino_data['distancia_km']} km<br>‚è±Ô∏è {destino_data['tempo_min']} min<br>{cache_icon} Cache",
                tooltip=f"{destino_data['destino']} - {destino_data['distancia_km']} km",
                icon=folium.Icon(color=cor, icon='map-marker', prefix='fa')
            ).add_to(m)
            
            folium.PolyLine(
                [[origem_lat, origem_lon], [dest_lat, dest_lon]],
                color=cor,
                weight=3,
                opacity=0.7,
                popup=f"{origem} ‚Üí {destino_data['destino']}: {destino_data['distancia_km']} km"
            ).add_to(m)
        
        return m

# ================================
# SISTEMA PRINCIPAL INTEGRADO
# ================================

class AdvancedDistanceSystemSQLite:
    def __init__(self):
        # Usar processador com SQLite + cache de dist√¢ncias
        self.parallel_processor = ParallelProcessor(max_workers=6)
        self.analytics = AnalyticsDashboard()
        self.mapper = InteractiveMapper()
        debugger.info("AdvancedDistanceSystemSQLite inicializado com tracking de erros")
        
    @debug_timing
    def analyze_spreadsheet_advanced(self, df):
        try:
            origem_col = df.columns[1] if len(df.columns) > 1 else None
            destino_cols = df.columns[2:] if len(df.columns) > 2 else []
            
            linhas_processaveis = []
            destinos_ignorados = 0  # *** NOVO: Contador de destinos ignorados ***
            
            for idx, row in df.iterrows():
                origem = self._clean_city_name(row[origem_col])
                
                if not origem:
                    continue
                
                destinos_linha = []
                for col in destino_cols:
                    destino = self._clean_city_name(row[col])
                    
                    # *** NOVO: Ignorar quando origem = destino ***
                    if destino and destino != origem:
                        destinos_linha.append(destino)
                    elif destino == origem:
                        destinos_ignorados += 1
                        debugger.info(f"Ignorando destino igual √† origem: {origem} = {destino}")
                
                if destinos_linha:
                    linhas_processaveis.append({
                        'linha_excel': idx + 1,
                        'origem': origem,
                        'destinos': destinos_linha,
                        'total_destinos': len(destinos_linha)
                    })
            
            analysis = {
                'total_rows_original': len(df),
                'linhas_processaveis': linhas_processaveis,
                'total_linhas_validas': len(linhas_processaveis),
                'total_calculos': sum(linha['total_destinos'] for linha in linhas_processaveis),
                'destinos_ignorados': destinos_ignorados,  # *** NOVO ***
                'estimativa_tempo_paralelo': sum(linha['total_destinos'] for linha in linhas_processaveis) * 0.05 / 60,
                'estimativa_tempo_sequencial': sum(linha['total_destinos'] for linha in linhas_processaveis) * 0.3 / 60
            }
            
            # *** NOVO: Log sobre destinos ignorados ***
            if destinos_ignorados > 0:
                debugger.info(f"An√°lise conclu√≠da: {destinos_ignorados} destinos ignorados (origem = destino)")
            
            return analysis
            
        except Exception as e:
            st.error(f"‚ùå Erro na an√°lise: {e}")
            return None
    
    def _clean_city_name(self, city_value):
        if pd.isna(city_value):
            return None
        
        city_clean = str(city_value).strip().upper()
        
        if city_clean in ['', 'NAN', 'NONE', 'NULL', '#N/A']:
            return None
        
        return city_clean.replace('  ', ' ')

# ================================
# PAINEL DE DEBUG COMPLETO
# ================================

def show_debug_panel():
    if not DEBUG_MODE:
        return
    
    with st.sidebar:
        st.markdown('<div class="debug-panel">', unsafe_allow_html=True)
        st.markdown("### üêõ Debug Panel Error Tracking")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üîß Debug Mode", "ON")
        with col2:
            st.metric("üßµ Threads", threading.active_count())
        
        if 'debug_logs' in st.session_state:
            total_logs = len(st.session_state.debug_logs)
            st.metric("üìù Total Logs", total_logs)
            
            if total_logs > 0:
                st.markdown("#### üìã Logs Recentes")
                for log in st.session_state.debug_logs[-5:]:
                    level_emoji = {
                        'DEBUG': 'üîç',
                        'INFO': '‚ÑπÔ∏è',
                        'WARNING': '‚ö†Ô∏è',
                        'ERROR': 'üö®'
                    }.get(log['level'], 'üìù')
                    
                    message = log['message'][:50] + "..." if len(log['message']) > 50 else log['message']
                    st.text(f"{level_emoji} [{log['timestamp']}] {message}")
        
        if st.button("üóëÔ∏è Limpar Logs Debug"):
            st.session_state.debug_logs = []
            st.success("Logs limpos!")
        
        st.markdown('</div>', unsafe_allow_html=True)

def main():
    """Fun√ß√£o principal COMPLETA com SQLite + Cache de Dist√¢ncias + Error Tracking"""
    
    debugger.info("=== INICIANDO APLICA√á√ÉO COM TRACKING DE ERROS ===")
    
    # Painel de debug
    show_debug_panel()
    
    # Header
    header_text = "üöÄ Sistema Avan√ßado de Dist√¢ncias - SQLite + Distance Cache + Error Tracking"
    if DEBUG_MODE:
        header_text += " (Debug Mode)"
    
    st.markdown(f"""
    <div class="main-header">
        <h1>{header_text}</h1>
        <h3>üíæ Cache SQLite ‚Ä¢ üõ£Ô∏è Cache de Dist√¢ncias ‚Ä¢ üö® Tracking de Erros ‚Ä¢ Analytics</h3>
        <p>Vers√£o 3.2.1 - Pro SQLite + Distance Cache + Error Tracking</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Inicializar sistema com SQLite + cache de dist√¢ncias + tracking de erros
    sistema = AdvancedDistanceSystemSQLite()
    
    # *** CONTROLE DE ESTADO DOS RESULTADOS ***
    # Inicializar resultados no session_state para evitar perda ap√≥s refresh
    if 'processamento_resultados' not in st.session_state:
        st.session_state.processamento_resultados = None
    if 'processamento_stats' not in st.session_state:
        st.session_state.processamento_stats = None
    if 'processamento_concluido' not in st.session_state:
        st.session_state.processamento_concluido = False
    
    # Sidebar com configura√ß√µes
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes Avan√ßadas")
        
        # Performance
        st.markdown("#### üöÄ Performance")
        max_workers = st.slider("üîß Workers Paralelos", 1, 8, 6)
        sistema.parallel_processor.max_workers = max_workers
        
        # Cache SQLite controls
        st.markdown("#### üíæ Cache SQLite + Dist√¢ncias")
        cache_stats = sistema.parallel_processor.cache.get_cache_stats()
        
        if cache_stats['total_requests'] > 0:
            st.metric("üìä Hit Rate Geral", f"{cache_stats['hit_rate']:.1f}%")
            st.metric("üóÉÔ∏è Entradas", cache_stats['total_entries'])
        
        # Painel de administra√ß√£o SQLite
        show_sqlite_admin_panel(sistema.parallel_processor.cache)
        
        # Toggles
        show_analytics = st.checkbox("üìä Mostrar Analytics", value=True)
        show_maps = st.checkbox("üó∫Ô∏è Mostrar Mapas", value=False)
        show_detailed_errors_panel = st.checkbox("üö® Mostrar Erros Detalhados", value=True)  # *** NOVO ***
        
        st.success("üíæ SQLite + üõ£Ô∏è Distance Cache + üö® Error Tracking Ativo")
    
    # Analytics Dashboard
    if show_analytics:
        sistema.analytics.show_performance_dashboard()
        sistema.analytics.show_cache_analytics(cache_stats)
    
    # Upload e processamento
    uploaded_file = st.file_uploader(
        "üìÅ Upload da Planilha Excel",
        type=['xlsx', 'xls'],
        help="Sistema com cache SQLite persistente, cache de dist√¢ncias E tracking de erros detalhado"
    )
    
    if uploaded_file is not None:
        debug_breakpoint("Arquivo carregado para processamento com error tracking", {
            'filename': uploaded_file.name,
            'size': uploaded_file.size,
            'cache_type': 'SQLite + Distance Cache + Error Tracking'
        })
        
        with st.spinner("üîç An√°lise avan√ßada da planilha..."):
            df = pd.read_excel(uploaded_file)
            analysis = sistema.analyze_spreadsheet_advanced(df)
        
        if analysis:
            st.success("‚úÖ Planilha analisada - Sistema SQLite + Cache de Dist√¢ncias + Error Tracking ativo!")
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("üìã Linhas V√°lidas", analysis['total_linhas_validas'])
            with col2:
                st.metric("üìè C√°lculos Totais", analysis['total_calculos'])
            with col3:
                st.metric("‚è±Ô∏è Tempo Est.", f"{analysis['estimativa_tempo_paralelo']:.1f} min")
            with col4:
                # *** NOVO: Mostrar destinos ignorados ***
                st.metric("üö´ Origem=Destino", analysis['destinos_ignorados'])
            
            # Info das melhorias
            if analysis['destinos_ignorados'] > 0:
                st.info(f"üö´ **{analysis['destinos_ignorados']} destinos ignorados** - quando origem = destino")
            
            st.info("üíæ **Cache SQLite + üõ£Ô∏è Cache de Dist√¢ncias + üö® Error Tracking ativos**")
            
            # Processamento
            if st.button("üöÄ Processar com Error Tracking", type="primary", use_container_width=True):
                
                debug_breakpoint("In√≠cio processamento com error tracking", {
                    'total_linhas': analysis['total_linhas_validas'],
                    'cache_type': 'SQLite + Distance Cache + Error Tracking',
                    'workers': max_workers
                })
                
                st.markdown("### ‚ö° Processamento Paralelo com Cache SQLite + Dist√¢ncias + Error Tracking")
                
                progress_bar = st.progress(0)
                status_info = st.empty()
                
                start_time = time.time()
                
                status_info.info("üöÄ Iniciando processamento com tracking de erros...")
                progress_bar.progress(20)
                
                with st.spinner("Executando processamento com cache SQLite + dist√¢ncias + error tracking..."):
                    resultados, stats_globais = sistema.parallel_processor.process_all_lines_parallel(
                        analysis['linhas_processaveis']
                    )
                
                progress_bar.progress(100)
                status_info.success("‚úÖ Processamento com error tracking conclu√≠do!")
                
                # *** SALVAR RESULTADOS NO SESSION STATE PARA EVITAR PERDA ***
                st.session_state.processamento_resultados = resultados
                st.session_state.processamento_stats = stats_globais
                st.session_state.processamento_concluido = True
                
                # Log analytics
                sistema.analytics.log_processamento(resultados, stats_globais)
    
    # *** EXIBIR RESULTADOS SE EXISTIREM (MESMO AP√ìS REFRESH) ***
    if st.session_state.processamento_concluido and st.session_state.processamento_resultados:
        resultados = st.session_state.processamento_resultados
        stats_globais = st.session_state.processamento_stats
        
        st.markdown("### üìä Resultados do Processamento SQLite + Distance Cache + Error Tracking")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("‚è±Ô∏è Tempo Total", f"{stats_globais['tempo_total']:.2f}s")
        with col2:
            st.metric("üöÄ Workers", stats_globais['workers_utilizados'])
        with col3:
            st.metric("‚úÖ Sucessos", stats_globais['total_sucessos'])
        with col4:
            st.metric("‚ùå Erros", stats_globais['total_erros'])
        with col5:
            # *** NOVO: Mostrar total de erros detalhados ***
            total_erros_detalhados = len(stats_globais.get('erros_detalhados', []))
            st.metric("üö® Erros Detalhados", total_erros_detalhados)
        
        # Taxa de sucesso
        taxa_final = (stats_globais['total_sucessos'] / max(1, stats_globais['total_sucessos'] + stats_globais['total_erros'])) * 100
        
        # Atualizar stats do cache
        updated_cache_stats = sistema.parallel_processor.cache.get_cache_stats()
        
        # *** NOVO: Informa√ß√£o sobre erros detalhados ***
        error_info = ""
        if total_erros_detalhados > 0:
            error_info = f"\nüö® **{total_erros_detalhados} erros espec√≠ficos** rastreados por origem-destino"
        
        st.success(f"""
        üöÄ **Sistema SQLite + Cache de Dist√¢ncias + Error Tracking - Performance M√°xima!**
        
        ‚ö° **Processamento conclu√≠do** em {stats_globais['tempo_total']:.2f}s  
        üíæ **Cache SQLite:** {updated_cache_stats['coord_hit_rate']:.1f}% hit rate coordenadas  
        üõ£Ô∏è **Cache Dist√¢ncias:** {updated_cache_stats['distance_hit_rate']:.1f}% hit rate dist√¢ncias  
        üìà **Taxa de Sucesso:** {taxa_final:.1f}%  
        üîß **{stats_globais['workers_utilizados']} workers** executando em paralelo{error_info}
        """)
        
        # *** NOVO: Painel de erros detalhados ***
        if show_detailed_errors_panel and stats_globais.get('erros_detalhados'):
            show_detailed_errors(stats_globais['erros_detalhados'])
        
        # Resultados detalhados
        st.markdown("### üìã Resultados Detalhados por Linha")
        
        for resultado in resultados:
            # *** NOVO: Indicar se h√° erros na linha ***
            erros_linha = len(resultado.get('erros_detalhados', []))
            erro_indicator = f" (üö® {erros_linha} erros)" if erros_linha > 0 else ""
            
            with st.expander(f"üéØ Linha {resultado['linha_excel']}: {resultado['origem']} ({resultado['sucessos']} sucessos{erro_indicator})"):
                
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    if resultado['status'] == 'concluido':
                        sucessos = [d for d in resultado['destinos_calculados'] if d['distancia_km'] is not None]
                        
                        if sucessos:
                            df_destinos = pd.DataFrame(sucessos)
                            df_destinos = df_destinos.sort_values('distancia_km')
                            df_destinos['ranking'] = range(1, len(df_destinos) + 1)
                            
                            # Indicar se veio do cache
                            df_destinos['origem_dados'] = df_destinos['status'].apply(
                                lambda x: 'üíæ Cache' if x == 'cache_hit' else 'üÜï API'
                            )
                            
                            df_display = df_destinos[['ranking', 'destino', 'distancia_km', 'tempo_min', 'origem_dados']].head(10)
                            df_display.columns = ['#', 'Destino', 'Dist√¢ncia (km)', 'Tempo (min)', 'Fonte']
                            
                            st.dataframe(df_display, use_container_width=True, hide_index=True)
                            
                            if resultado['destino_mais_proximo']:
                                st.success(f"üèÜ **Mais Pr√≥ximo:** {resultado['destino_mais_proximo']} - {resultado['km_mais_proximo']} km")
                        
                        # *** NOVO: Mostrar erros da linha se houver ***
                        if resultado.get('erros_detalhados'):
                            st.markdown("**üö® Erros nesta linha:**")
                            for erro in resultado['erros_detalhados']:
                                emoji_tipo = {
                                    'geocoding_origem': 'üè†',
                                    'geocoding_destino': 'üéØ',
                                    'calculo_distancia': 'üõ£Ô∏è'
                                }.get(erro['tipo'], '‚ùå')
                                st.error(f"{emoji_tipo} {erro['origem']} ‚Üí {erro['destino']}: {erro['erro']}")
                
                with col2:
                    st.metric("‚è±Ô∏è Tempo", f"{resultado['tempo_processamento']}s")
                    st.metric("üìä Total", resultado['total_destinos'])
                    st.metric("‚úÖ Sucessos", resultado['sucessos'])
                    st.metric("‚ùå Erros", resultado['erros'])
                    
                    # *** NOVO: Mostrar quantidade de erros detalhados ***
                    if resultado.get('erros_detalhados'):
                        st.metric("üö® Erros Detalhados", len(resultado['erros_detalhados']))
                    
                    st.text("üíæ Cache: SQLite")
                    st.text("üõ£Ô∏è Dist√¢ncias: Cache")
                    st.text("üö® Error Tracking: ON")
                    
                    if show_maps and resultado['status'] == 'concluido':
                        mapa_linha = sistema.mapper.create_route_map(resultado)
                        if mapa_linha:
                            st.markdown("**üó∫Ô∏è Mapa:**")
                            st_folium(mapa_linha, width=300, height=200)
        
        # Download
        st.markdown("### üíæ Download do Relat√≥rio Error Tracking")
        
        excel_data = create_advanced_excel_error_tracking(resultados, stats_globais, updated_cache_stats)
        
        if excel_data:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"relatorio_error_tracking_{timestamp}.xlsx"
            
            st.download_button(
                label="üì• Baixar Relat√≥rio Error Tracking (Excel)",
                data=excel_data,
                file_name=filename,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                type="primary",
                use_container_width=True
            )
            
            st.info("""
            üìä **Relat√≥rio Error Tracking cont√©m:**
            - üìã Resultados detalhados por linha
            - üö® Lista completa de erros por origem-destino
            - üìä Estat√≠sticas de tipos de erro
            - üíæ M√©tricas de cache SQLite e dist√¢ncias
            - ‚ö° Analytics de performance
            """)
        
        # Bot√£o para limpar resultados
        if st.button("üóëÔ∏è Limpar Resultados", help="Remove os resultados da tela"):
            st.session_state.processamento_resultados = None
            st.session_state.processamento_stats = None
            st.session_state.processamento_concluido = False

def create_advanced_excel_error_tracking(resultados, stats_globais, cache_stats):
    """Cria Excel com informa√ß√µes espec√≠ficas de error tracking"""
    output = BytesIO()
    
    try:
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Aba principal
            relatorio_completo = []
            for resultado in resultados:
                if resultado['status'] == 'concluido':
                    sucessos = [d for d in resultado['destinos_calculados'] if d['distancia_km'] is not None]
                    
                    for i, dest_data in enumerate(sucessos, 1):
                        relatorio_completo.append({
                            'Linha_Excel': resultado['linha_excel'],
                            'Origem': resultado['origem'],
                            'Destino': dest_data['destino'],
                            'Distancia_KM': dest_data['distancia_km'],
                            'Tempo_Minutos': dest_data['tempo_min'],
                            'Ranking_na_Linha': i,
                            'Eh_Mais_Proximo': 'SIM' if dest_data['destino'] == resultado['destino_mais_proximo'] else 'N√ÉO',
                            'Origem_Dados': 'Cache' if dest_data.get('status') == 'cache_hit' else 'API',
                            'Tempo_Processamento_Linha': resultado['tempo_processamento']
                        })
            
            if relatorio_completo:
                df_completo = pd.DataFrame(relatorio_completo)
                df_completo.to_excel(writer, sheet_name='Resultados_Sucessos', index=False)
            
            # *** NOVA ABA: Erros Detalhados ***
            erros_detalhados = stats_globais.get('erros_detalhados', [])
            if erros_detalhados:
                df_erros = pd.DataFrame(erros_detalhados)
                df_erros['Par_Origem_Destino'] = df_erros['origem'] + ' ‚Üí ' + df_erros['destino']
                df_erros = df_erros[['tipo', 'origem', 'destino', 'Par_Origem_Destino', 'erro']]
                df_erros.columns = ['Tipo_Erro', 'Origem', 'Destino', 'Par_Origem_Destino', 'Descricao_Erro']
                df_erros.to_excel(writer, sheet_name='Erros_Detalhados', index=False)
                
                # Estat√≠sticas de erros
                tipos_erro = df_erros['Tipo_Erro'].value_counts().reset_index()
                tipos_erro.columns = ['Tipo_Erro', 'Quantidade']
                tipos_erro.to_excel(writer, sheet_name='Estatisticas_Erros', index=False)
            
            # M√©tricas espec√≠ficas do error tracking
            performance_data = {
                'M√©trica': [
                    'Tempo Total de Processamento (s)',
                    'Cache Type',
                    'Error Tracking Ativo',
                    'Total de Erros Detalhados',
                    'Cache Hit Rate Coordenadas (%)',
                    'Cache Hit Rate Dist√¢ncias (%)',
                    'Hit Rate Geral (%)',
                    'Total Coordenadas Cache',
                    'Total Dist√¢ncias Cache',
                    'Total Entradas Cache',
                    'TTL Coordenadas (h)',
                    'TTL Dist√¢ncias (h)',
                    'Database Path',
                    'Workers Utilizados',
                    'Linhas Processadas',
                    'Total de Sucessos',
                    'Total de Erros',
                    'Taxa de Sucesso (%)',
                    'Sistema de Coordenadas',
                    'API de Rotas'
                ],
                'Valor': [
                    stats_globais['tempo_total'],
                    'SQLite + Distance Cache + Error Tracking',
                    'SIM',
                    len(erros_detalhados),
                    cache_stats['coord_hit_rate'],
                    cache_stats['distance_hit_rate'],
                    cache_stats['hit_rate'],
                    cache_stats['total_coordinates'],
                    cache_stats['total_distances'],
                    cache_stats['total_entries'],
                    cache_stats['ttl_hours'],
                    cache_stats['distance_ttl_hours'],
                    cache_stats.get('database_path', 'N/A'),
                    stats_globais['workers_utilizados'],
                    stats_globais['linhas_processadas'],
                    stats_globais['total_sucessos'],
                    stats_globais['total_erros'],
                    round((stats_globais['total_sucessos'] / max(1, stats_globais['total_sucessos'] + stats_globais['total_erros'])) * 100, 2),
                    'Nominatim (OpenStreetMap)',
                    'OSRM (Open Source Routing Machine)'
                ]
            }
            
            df_performance = pd.DataFrame(performance_data)
            df_performance.to_excel(writer, sheet_name='Metricas_Error_Tracking', index=False)
        
        return output.getvalue()
        
    except Exception as e:
        st.error(f"‚ùå Erro ao criar Excel: {e}")
        return None

if __name__ == "__main__":
    main()